{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "data = []\n",
        "for page in range(1, 21):  # Loop from page 1 to 20\n",
        "    url = f'http://quotes.toscrape.com/page/{page}/'\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check that the request was successful\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    quotes = soup.find_all('div', class_='quote')\n",
        "    for quote in quotes:\n",
        "        text = quote.find('span', class_='text').get_text()\n",
        "        author = quote.find('small', class_='author').get_text()\n",
        "        tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
        "        data.append([text, author, ', '.join(tags)])\n",
        "df = pd.DataFrame(data, columns=['Quote', 'Author', 'Tags'])\n",
        "df.to_csv('quotes_20.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Quotes from 20 pages have been successfully saved to 'quotes.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4fpkpfQjnIk",
        "outputId": "8147ad1b-67e9-4c5a-ec7f-699954fa23d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quotes from 20 pages have been successfully saved to 'quotes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd  # Import pandas\n",
        "# Step 1: Send a request to the website\n",
        "url = 'http://quotes.toscrape.com/'\n",
        "response = requests.get(url)\n",
        "print(response.raise_for_status())  # Check that the request was successful\n",
        "# Step 2: Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Step 3: Find all quote elements\n",
        "quotes = soup.find_all('div', class_='quote')\n",
        "# Step 4: Extract data and store it in a list\n",
        "data = []\n",
        "for quote in quotes:\n",
        "    text = quote.find('span', class_='text').get_text()\n",
        "    author = quote.find('small', class_='author').get_text()\n",
        "    tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
        "    data.append([text, author, ', '.join(tags)])\n",
        "# Step 5: Write the data to a CSV file using pandas\n",
        "df = pd.DataFrame(data, columns=['Quote', 'Author', 'Tags'])  # Create DataFrame\n",
        "df.to_csv('quotes.csv')  # Export to CSV\n",
        "df\n",
        "\n",
        "print(\"Quotes have been successfully saved to 'quotes.csv'\")"
      ],
      "metadata": {
        "id": "yPMgb82JjD9t",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63f2bf1-7b14-48cd-9111-880e01ff50fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Quotes have been successfully saved to 'quotes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkSmxm2VMram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Step 1: Send a request to the website\n",
        "url = 'http://quotes.toscrape.com/'\n",
        "response = requests.get(url)\n",
        "print(response.raise_for_status())  # Check that the request was successful\n",
        "# Step 2: Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Step 3: Find all quote elements\n",
        "quotes = soup.find_all('div', class_='quote')\n",
        "\n",
        "# Step 4: Extract data and store it in a list\n",
        "data = []\n",
        "for quote in quotes:\n",
        "    text = quote.find('span', class_='text').get_text()\n",
        "    author = quote.find('small', class_='author').get_text()\n",
        "    tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
        "    data.append([text, author, ', '.join(tags)])\n"
      ],
      "metadata": {
        "id": "mIFudoOobZg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Write the data to a CSV file\n",
        "with open('quotes.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Quote', 'Author', 'Tags'])\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "id": "UMYG5ILVdILU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Step 1: Send a request to the website\n",
        "url = 'http://books.toscrape.com/'\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Check that the request was successful\n",
        "\n",
        "# Step 2: Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 3: Find all product elements\n",
        "products = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "# Step 4: Extract data and store it in a list\n",
        "data = []\n",
        "for product in products:\n",
        "    name = product.find('h3').find('a')['title']\n",
        "    price = product.find('p', class_='price_color').get_text()\n",
        "    availability = product.find('p', class_='instock availability').get_text(strip=True)\n",
        "    data.append([name, price, availability])\n",
        "# Step 5: create df and make a CSV file\n",
        "df = pd.DataFrame(data, columns=['Name', 'Price', 'Availability'])\n",
        "df.to_csv('books.csv', index=False, encoding='utf-8')\n",
        "print(\"Bookss have been successfully saved to 'books.csv'\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omn0iqj0ztrs",
        "outputId": "d373e4f7-3a32-4077-8755-f7af6e8e9482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bookss have been successfully saved to 'books.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
        "\n",
        "# List to store data from all pages\n",
        "all_data = []\n",
        "\n",
        "# Loop through the first 50 pages\n",
        "for page in range(1, 51):\n",
        "    # Construct the URL for the current page\n",
        "    url = base_url.format(page)\n",
        "\n",
        "    # Step 1: Send a request to the website\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check that the request was successful\n",
        "\n",
        "    # Step 2: Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Step 3: Find all product elements\n",
        "    products = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Step 4: Extract data from each product and add it to the list\n",
        "    for product in products:\n",
        "        name = product.find('h3').find('a')['title']\n",
        "        price = product.find('p', class_='price_color').get_text()\n",
        "        availability = product.find('p', class_='instock availability').get_text(strip=True)\n",
        "        all_data.append([name, price, availability])\n",
        "\n",
        "# Step 5: Create a DataFrame and save it to a CSV file\n",
        "df = pd.DataFrame(all_data, columns=['Name', 'Price', 'Availability'])\n",
        "df.to_csv('products.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Products from 50 pages have been successfully saved to 'products.csv'\")"
      ],
      "metadata": {
        "id": "vAthVJDS3SQH",
        "outputId": "d3c2de30-0b95-4627-8d39-1351fa7cec30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products from 20 pages have been successfully saved to 'products.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sKvCkYYJ1HEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
        "\n",
        "# List to store data from all pages\n",
        "all_data = []\n",
        "\n",
        "# Loop through the first 10 pages\n",
        "for page in range(1, 11):\n",
        "    # Construct the URL for the current page\n",
        "    url = base_url.format(page)\n",
        "\n",
        "    # Step 1: Send a request to the website\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check that the request was successful\n",
        "\n",
        "    # Step 2: Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Step 3: Find all product elements\n",
        "    products = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Step 4: Extract data from each product and add it to the list\n",
        "    for product in products:\n",
        "        name = product.find('h3').find('a')['title']\n",
        "        price = product.find('p', class_='price_color').get_text()\n",
        "        availability = product.find('p', class_='instock availability').get_text(strip=True)\n",
        "        all_data.append([name, price, availability])\n",
        "\n",
        "# Step 5: Write the collected data to a CSV file\n",
        "with open('products.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Name', 'Price', 'Availability'])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(\"Products from 10 pages have been successfully saved to 'products.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF4oKYQTGuqJ",
        "outputId": "6935fb9a-9593-4146-bfd3-baa3af1b69ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products from 10 pages have been successfully saved to 'products.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using** **API**"
      ],
      "metadata": {
        "id": "T3PVj3db4AQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIFxB2ff4DBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "# Step 1: Define the endpoint\n",
        "url = 'https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum&vs_currencies=usd'\n",
        "\n",
        "# Step 2: Send a request to the CoinGecko API\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Step 3: Extract data\n",
        "crypto_data = [\n",
        "    ['Bitcoin', data['bitcoin']['usd']],\n",
        "    ['Ethereum', data['ethereum']['usd']]\n",
        "]\n",
        "\n",
        "# Step 4: Write the data to a CSV file\n",
        "with open('crypto_prices.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Cryptocurrency', 'Price (USD)'])\n",
        "    writer.writerows(crypto_data)\n",
        "\n",
        "print(\"Cryptocurrency prices have been successfully saved to 'crypto_prices.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjXGdGsf0JG1",
        "outputId": "84a81f7c-bf27-4588-f790-855f4276e71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cryptocurrency prices have been successfully saved to 'crypto_prices.csv'\n"
          ]
        }
      ]
    }
  ]
}